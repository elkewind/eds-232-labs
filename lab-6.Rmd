---
title: "Lab6"
author: Elke Windschitl
date: "2023-03-01"
output: html_document
---

```{r}
library(readr)
library(tidyverse)
library(tidymodels)
library(xgboost)
library(tictoc)
```

## Case Study Eel Species Distribution Modeling

This week's lab follows a modeling project described by Elith et al. (2008) (Supplementary Reading)

## Data

Grab the model training data set from the class Git:

data/eel.model.data.csv

```{r}
urlfile <- "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/eel.model.data.csv"

eel_data <- read_csv(url(urlfile)) %>% 
  select(-Site)
eel_data$Angaus <- as.factor(eel_data$Angaus)
```

### Split and Resample

Split the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus

```{r}
# Stratified sampling with the rsample package
set.seed(123) #set a seed for reproducibility
split <- initial_split(data = eel_data, 
                       prop = .7, 
                       strata = "Angaus")
split
eel_train <- training(split) 
eel_test  <- testing(split)

# Set up cross validation
cv_folds <- eel_train %>% 
  vfold_cv(v=10, strata = "Angaus")
```

### Preprocess

Create a recipe to prepare your data for the XGBoost model. We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis

```{r}
eel_rec <- recipe(Angaus ~ ., data = eel_train) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% 
  prep(training = eel_train, retain = TRUE)

# bake to check
baked_eel <- bake(eel_rec, eel_train)

# create matrices
# X <- as.matrix(baked_eel[setdiff(names(baked_eel), "Angaus")])
# Y <- baked_eel$Angaus
```


```{r}
# eel_cv_folds <- 
#   recipes::bake(
#     eel_rec, 
#     new_data = eel_train) %>%  
#   rsample::vfold_cv(v = 5)
```

## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined on Monday, first we conduct tuning on just the learn_rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}
eel_spec <- parsnip::boost_tree(mode = "classification",
                                engine = "xgboost",
                                trees = 3000,
                                learn_rate = tune())

```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

```{r}
tic()
eel_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))
eel_grid

wf_eel_tune <- workflow() %>% 
  add_recipe(eel_rec) %>% 
  add_model(eel_spec)
toc()
```

```{r}
tic()
doParallel::registerDoParallel()

eel_rs <- tune_grid(
  wf_eel_tune,
  Angaus~.,
  resamples = cv_folds,
  grid = eel_grid,
  metrics = metric_set(accuracy)
)
toc()
eel_rs
```

3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
eel_rs %>%
  tune::show_best(metric = "accuracy") %>%
  knitr::kable()

eel_best_learn <- eel_rs %>%
  tune::select_best("accuracy")

knitr::kable(eel_best_learn)

eel_model <- eel_spec %>% 
  finalize_model(eel_best_learn)
```

### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.
```{r}
eel_spec2 <- parsnip::boost_tree(mode = "classification",
                                engine = "xgboost",
                                trees = 3000,
                                learn_rate = eel_best_learn$learn_rate,
                                min_n = tune(),
                                tree_depth = tune(),
                                mtry = tune()                                
                                )
```


2.  Set up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space

```{r}
eel_params <- dials::parameters(
  min_n(),
  tree_depth(),
  finalize(mtry(),select(baked_eel,-Angaus))
)

eel_grid2 <- dials::grid_max_entropy(eel_params, size = 30)
knitr::kable(head(eel_grid2))

wf_eel_tune2 <- workflow() %>% 
  add_recipe(eel_rec) %>% 
  add_model(eel_spec2)
```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.
```{r}
tic()
doParallel::registerDoParallel()

eel_rs2 <- tune_grid(
  wf_eel_tune2,
  Angaus~.,
  resamples = cv_folds,
  grid = eel_grid2,
  metrics = metric_set(accuracy)
)
toc()
eel_rs2
```

```{r}
eel_rs2 %>%
  tune::show_best(metric = "accuracy") %>%
  knitr::kable()

eel_best_trees <- eel_rs2 %>%
  tune::select_best("accuracy")

knitr::kable(eel_best_trees)

eel_model2 <- eel_spec2 %>% 
  finalize_model(eel_best_trees)
```

### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.
```{r}
eel_spec3 <- parsnip::boost_tree(mode = "classification",
                                engine = "xgboost",
                                trees = 3000,
                                learn_rate = eel_best_learn$learn_rate,
                                min_n = eel_best_trees$min_n,
                                tree_depth = eel_best_trees$tree_depth,
                                mtry = eel_best_trees$mtry,                   
                                loss_reduction = tune(),
                                stop_iter = tune()
                                )
```


2.  Set up a tuning grid. Use grid_max_entropy() again.
```{r}
eel_params2 <- dials::parameters(
  loss_reduction(),
  stop_iter()
)

eel_grid3 <- dials::grid_max_entropy(eel_params2, size = 30)
knitr::kable(head(eel_grid3))

wf_eel_tune3 <- workflow() %>% 
  add_recipe(eel_rec) %>% 
  add_model(eel_spec3)
```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.
```{r}
tic()
doParallel::registerDoParallel()

eel_rs3 <- tune_grid(
  wf_eel_tune3,
  Angaus~.,
  resamples = cv_folds,
  grid = eel_grid3,
  metrics = metric_set(accuracy)
)
toc()
eel_rs3
```

```{r}
eel_rs3 %>%
  tune::show_best(metric = "accuracy") %>%
  knitr::kable()

eel_best_stoch <- eel_rs3 %>%
  tune::select_best("accuracy")

knitr::kable(eel_best_stoch)

eel_model3 <- eel_spec3 %>% 
  finalize_model(eel_best_stoch)
```

## Finalize workflow and make final prediction

1.  Assemble your final workflow will all of your optimized parameters and do a final fit.
```{r}
eel_final_spec <- parsnip::boost_tree(mode = "classification",
                                engine = "xgboost",
                                trees = 3000,
                                learn_rate = eel_best_learn$learn_rate,
                                min_n = eel_best_trees$min_n,
                                tree_depth = eel_best_trees$tree_depth,
                                mtry = eel_best_trees$mtry,                   
                                loss_reduction = eel_best_stoch$loss_reduction,
                                stop_iter = eel_best_stoch$stop_iter
                                )
wf_eel_final <- workflow() %>% 
  add_recipe(eel_rec) %>% 
  add_model(eel_final_spec)

final_eel_fit <- last_fit(eel_final_spec, Angaus~., split) # does training fit then final prediction as well
final_eel_fit$.predictions
final_eel_fit$.metrics

eel_test_rs <- cbind(eel_test, final_eel_fit$.predictions)
eel_test_rs <- eel_test_rs[,-1]

cm<- eel_test_rs %>% yardstick::conf_mat(truth = Angaus, estimate = .pred_class) 
autoplot(cm, type = "heatmap") 
```

2.  How well did your model perform? What types of errors did it make?

## Fit your model the evaluation data and compare performance

1.  Now fit your final model to the big dataset: data/eval.data.csv

2.  How does your model perform on this data?

3.  How do your results compare to those of Elith et al.?

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?
